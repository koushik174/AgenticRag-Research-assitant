{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "***AgentRAG: Autonomous Research Assistant***\n",
        "This notebook implements an intelligent agent that can research topics, retrieve information from the web, and provide comprehensive answers to complex questions."
      ],
      "metadata": {
        "id": "N2Dh3eIbhLl9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KSt2RbidbfxU",
        "outputId": "a80f9936-8b4a-48d7-e70d-475ce2eb3d63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.26.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.61.1)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.10.6)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.27.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m62.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m53.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 python-dotenv-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install numpy pandas torch transformers scikit-learn requests openai python-dotenv beautifulsoup4 tqdm"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "os.environ[\"SERPER_API_KEY\"] = \"\""
      ],
      "metadata": {
        "id": "ArHQs6yobnWk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Imports and Initial Setup\n",
        "Let's import all the necessary libraries:"
      ],
      "metadata": {
        "id": "ym1D48SkhYzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "import json\n",
        "import numpy as np\n",
        "import requests\n",
        "import pandas as pd\n",
        "from typing import List, Dict, Any, Tuple, Optional\n",
        "from dataclasses import dataclass, field\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import time\n",
        "import re\n",
        "from collections import deque\n",
        "import warnings\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Suppress specific warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"transformers\")"
      ],
      "metadata": {
        "id": "yOEpsMA5hj5G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class AgentConfig:\n",
        "    \"\"\"Configuration for the Agentic RAG system\"\"\"\n",
        "    model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "    chunk_size: int = 512\n",
        "    chunk_overlap: int = 50\n",
        "    top_k: int = 5\n",
        "    similarity_threshold: float = 0.7\n",
        "    max_iterations: int = 3\n",
        "    api_key: str = os.environ.get(\"OPENAI_API_KEY\", \"your-openai-api-key\")\n",
        "    serper_api_key: str = os.environ.get(\"SERPER_API_KEY\", \"your-serper-api-key\")\n",
        "\n",
        "@dataclass\n",
        "class Document:\n",
        "    \"\"\"Representation of a document with content and metadata\"\"\"\n",
        "    content: str\n",
        "    metadata: Dict[str, Any] = field(default_factory=dict)\n",
        "    embedding: Optional[np.ndarray] = None\n",
        "    chunks: List[Dict[str, Any]] = field(default_factory=list)\n",
        "\n",
        "    def __post_init__(self):\n",
        "        self.id = self.metadata.get(\"id\", hash(self.content) % 10000)"
      ],
      "metadata": {
        "id": "2rSozDi7hohk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Vector Store Implementation**\n",
        "\n",
        "The Vector Store handles document embeddings and similarity searches:"
      ],
      "metadata": {
        "id": "-wFLP5yAhrMp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStore:\n",
        "    \"\"\"In-memory vector store for document embeddings\"\"\"\n",
        "    def __init__(self, config: AgentConfig):\n",
        "        self.config = config\n",
        "        self.documents: List[Document] = []\n",
        "        self.chunks: List[Dict[str, Any]] = []\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        print(f\"Loading embedding model on {self.device}...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(config.model_name)\n",
        "        self.model = AutoModel.from_pretrained(config.model_name).to(self.device)\n",
        "        print(\"Embedding model loaded successfully!\")\n",
        "\n",
        "    def _get_embedding(self, text: str) -> np.ndarray:\n",
        "        \"\"\"Generate embedding for text using the embedding model\"\"\"\n",
        "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model(**inputs)\n",
        "\n",
        "        # Mean pooling to get sentence embedding\n",
        "        token_embeddings = outputs.last_hidden_state\n",
        "        attention_mask = inputs['attention_mask']\n",
        "        input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
        "        embedding = torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
        "        return embedding[0].cpu().numpy()\n",
        "\n",
        "    def _chunk_document(self, document: Document) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Split document into chunks with overlap\"\"\"\n",
        "        text = document.content\n",
        "        chunks = []\n",
        "\n",
        "        for i in range(0, len(text), self.config.chunk_size - self.config.chunk_overlap):\n",
        "            chunk_text = text[i:i + self.config.chunk_size]\n",
        "            if len(chunk_text) < 50:  # Skip very small chunks\n",
        "                continue\n",
        "\n",
        "            chunk = {\n",
        "                \"text\": chunk_text,\n",
        "                \"embedding\": self._get_embedding(chunk_text),\n",
        "                \"doc_id\": document.id,\n",
        "                \"chunk_id\": len(chunks),\n",
        "                \"metadata\": document.metadata\n",
        "            }\n",
        "            chunks.append(chunk)\n",
        "\n",
        "        return chunks\n",
        "\n",
        "    def add_document(self, document: Document) -> None:\n",
        "        \"\"\"Add document to the store, chunk it, and compute embeddings\"\"\"\n",
        "        document.embedding = self._get_embedding(document.content)\n",
        "        document.chunks = self._chunk_document(document)\n",
        "        self.documents.append(document)\n",
        "        self.chunks.extend(document.chunks)\n",
        "\n",
        "    def similarity_search(self, query: str, top_k: int = None) -> List[Dict[str, Any]]:\n",
        "        \"\"\"Find most similar chunks to the query\"\"\"\n",
        "        if top_k is None:\n",
        "            top_k = self.config.top_k\n",
        "\n",
        "        # Handle empty chunks case\n",
        "        if not self.chunks:\n",
        "            return []\n",
        "\n",
        "        query_embedding = self._get_embedding(query)\n",
        "        chunk_embeddings = np.array([chunk[\"embedding\"] for chunk in self.chunks])\n",
        "\n",
        "        similarities = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
        "        top_indices = np.argsort(similarities)[-min(top_k, len(similarities)):][::-1]\n",
        "\n",
        "        results = []\n",
        "        for idx in top_indices:\n",
        "            if similarities[idx] >= self.config.similarity_threshold:\n",
        "                results.append({\n",
        "                    \"chunk\": self.chunks[idx],\n",
        "                    \"similarity\": float(similarities[idx])\n",
        "                })\n",
        "\n",
        "        return results"
      ],
      "metadata": {
        "id": "C3pi4PGfhpPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Web Retriever Implementation**\n",
        "The Web Retriever handles searching the web and extracting content from web pages:"
      ],
      "metadata": {
        "id": "Jzo2fg5hh2Mc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WebRetriever:\n",
        "    \"\"\"Component for retrieving information from the web using real search APIs\"\"\"\n",
        "\n",
        "    def __init__(self, config: AgentConfig):\n",
        "        self.config = config\n",
        "        self.serper_api_key = os.environ.get(\"SERPER_API_KEY\", config.serper_api_key)\n",
        "\n",
        "    def search(self, query: str, num_results: int = 3) -> List[Document]:\n",
        "        \"\"\"Search the web for information related to the query using Serper API\"\"\"\n",
        "        try:\n",
        "            # Use Serper.dev Google Search API\n",
        "            headers = {\n",
        "                'X-API-KEY': self.serper_api_key,\n",
        "                'Content-Type': 'application/json'\n",
        "            }\n",
        "\n",
        "            payload = {\n",
        "                \"q\": query,\n",
        "                \"num\": num_results\n",
        "            }\n",
        "\n",
        "            response = requests.post(\n",
        "                'https://google.serper.dev/search',\n",
        "                headers=headers,\n",
        "                json=payload\n",
        "            )\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                print(f\"Search API error: Status code {response.status_code}\")\n",
        "                return []\n",
        "\n",
        "            search_results = response.json()\n",
        "            documents = []\n",
        "\n",
        "            # Process organic search results\n",
        "            if \"organic\" in search_results:\n",
        "                for result in search_results[\"organic\"][:num_results]:\n",
        "                    # Try to extract content from each URL\n",
        "                    title = result.get(\"title\", \"\")\n",
        "                    snippet = result.get(\"snippet\", \"\")\n",
        "                    url = result.get(\"link\", \"\")\n",
        "\n",
        "                    # Start with the snippet as minimal content\n",
        "                    content = f\"Title: {title}\\n\\nExcerpt: {snippet}\"\n",
        "\n",
        "                    # Try to get full content\n",
        "                    full_doc = self.extract_text_from_url(url)\n",
        "                    if full_doc and len(full_doc.content) > len(content):\n",
        "                        documents.append(full_doc)\n",
        "                    else:\n",
        "                        # Use snippet if extraction failed\n",
        "                        documents.append(Document(\n",
        "                            content=content,\n",
        "                            metadata={\"source\": url, \"title\": title}\n",
        "                        ))\n",
        "\n",
        "            return documents\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during web search: {e}\")\n",
        "            return []\n",
        "\n",
        "    def extract_text_from_url(self, url: str) -> Optional[Document]:\n",
        "        \"\"\"Extract text content from a URL using requests and basic HTML parsing\"\"\"\n",
        "        try:\n",
        "            headers = {\n",
        "                \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, headers=headers, timeout=10)\n",
        "\n",
        "            if response.status_code != 200:\n",
        "                return None\n",
        "\n",
        "            # Use BeautifulSoup for content extraction\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            # Remove script and style elements\n",
        "            for script in soup([\"script\", \"style\", \"nav\", \"footer\", \"header\"]):\n",
        "                script.extract()\n",
        "\n",
        "            # Get text content\n",
        "            text = soup.get_text(separator=\"\\n\", strip=True)\n",
        "\n",
        "            # Clean up text (remove extra newlines, etc.)\n",
        "            lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "            content = \"\\n\".join(lines)\n",
        "\n",
        "            # Limit content length to avoid extremely long documents\n",
        "            max_length = 10000\n",
        "            if len(content) > max_length:\n",
        "                content = content[:max_length] + \"... [content truncated]\"\n",
        "\n",
        "            return Document(\n",
        "                content=content,\n",
        "                metadata={\"source\": url, \"title\": soup.title.string if soup.title else url}\n",
        "            )\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error extracting text from {url}: {str(e)}\")\n",
        "            return None"
      ],
      "metadata": {
        "id": "AcFvEgyQh7nC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LLM Interface**\n",
        "\n",
        "The LLM Interface connects to OpenAI's API to generate responses:"
      ],
      "metadata": {
        "id": "Z1Y-D4ueiCHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMInterface:\n",
        "    \"\"\"Interface to the OpenAI API for LLM capabilities\"\"\"\n",
        "\n",
        "    def __init__(self, config: AgentConfig):\n",
        "        self.config = config\n",
        "        self.api_key = os.environ.get(\"OPENAI_API_KEY\", config.api_key)\n",
        "\n",
        "    def generate(self, prompt: str, temperature: float = 0.7, max_tokens: int = 800) -> str:\n",
        "        \"\"\"Generate text using OpenAI's API\"\"\"\n",
        "        try:\n",
        "            import openai\n",
        "\n",
        "            # Set the API key\n",
        "            openai.api_key = self.api_key\n",
        "\n",
        "            # Make API call\n",
        "            response = openai.chat.completions.create(\n",
        "                model=\"gpt-3.5-turbo\",  # You can use gpt-4 for higher quality\n",
        "                messages=[\n",
        "                    {\"role\": \"system\", \"content\": \"You are a helpful research assistant that provides factual, concise information.\"},\n",
        "                    {\"role\": \"user\", \"content\": prompt}\n",
        "                ],\n",
        "                temperature=temperature,\n",
        "                max_tokens=max_tokens,\n",
        "                top_p=1.0,\n",
        "                frequency_penalty=0.0,\n",
        "                presence_penalty=0.0\n",
        "            )\n",
        "\n",
        "            # Extract and return the response text\n",
        "            return response.choices[0].message.content.strip()\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error generating LLM response: {e}\")\n",
        "\n",
        "            # Fallback response if API call fails\n",
        "            fallback_msg = f\"Error generating response: {str(e)}. \"\n",
        "            fallback_msg += \"Please check your API key and connection. \"\n",
        "\n",
        "            if \"RateLimitError\" in str(e):\n",
        "                fallback_msg += \"You've hit a rate limit. Please try again in a minute.\"\n",
        "            elif \"AuthenticationError\" in str(e):\n",
        "                fallback_msg += \"Authentication failed. Please check your API key.\"\n",
        "\n",
        "            return fallback_msg"
      ],
      "metadata": {
        "id": "0fLxzZPah-sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First cell - Class definition with basic methods\n",
        "class AgentRAG:\n",
        "    \"\"\"Main agent class that orchestrates the RAG process\"\"\"\n",
        "\n",
        "    def __init__(self, config: AgentConfig = None):\n",
        "        print(\"Initializing AgentRAG...\")\n",
        "        self.config = config or AgentConfig()\n",
        "        self.vector_store = VectorStore(self.config)\n",
        "        self.web_retriever = WebRetriever(self.config)\n",
        "        self.llm = LLMInterface(self.config)\n",
        "        self.memory = deque(maxlen=10)  # Short-term memory for conversation\n",
        "        print(\"AgentRAG initialized!\")\n",
        "\n",
        "    def add_to_memory(self, item: Dict[str, Any]) -> None:\n",
        "        \"\"\"Add an interaction to the agent's memory\"\"\"\n",
        "        self.memory.append(item)\n",
        "\n",
        "    def _format_context(self, chunks: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Format retrieved chunks into context for the LLM\"\"\"\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"Source: {chunk['chunk']['metadata'].get('source', 'Unknown')}\\n\"\n",
        "            f\"Content: {chunk['chunk']['text']}\"\n",
        "            for chunk in chunks\n",
        "        ])\n",
        "        return context\n",
        "\n",
        "    def _generate_search_queries(self, user_query: str) -> List[str]:\n",
        "        \"\"\"Generate search queries based on the user query\"\"\"\n",
        "        # In production, use LLM to generate these\n",
        "        base_query = user_query.strip(\"?\").lower()\n",
        "        return [\n",
        "            base_query,\n",
        "            f\"latest information about {base_query}\",\n",
        "            f\"{base_query} explanation\"\n",
        "        ]\n",
        "\n",
        "    def _needs_web_search(self, query: str, results: List[Dict[str, Any]]) -> bool:\n",
        "        \"\"\"Determine if web search is needed based on query and existing results\"\"\"\n",
        "        if not results:\n",
        "            return True\n",
        "\n",
        "        # Check if results are relevant enough\n",
        "        avg_similarity = sum(r[\"similarity\"] for r in results) / len(results) if results else 0\n",
        "        return avg_similarity < 0.75\n",
        "\n",
        "    # Include process_query and recursive_research methods here too\n",
        "    def process_query(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process a user query through the full agentic RAG pipeline\"\"\"\n",
        "        # 1. Check existing knowledge\n",
        "        search_results = self.vector_store.similarity_search(query)\n",
        "\n",
        "        # 2. Always perform web search for the first query or if needed\n",
        "        if not search_results or self._needs_web_search(query, search_results):\n",
        "            print(\"Retrieving information from the web...\")\n",
        "            search_queries = self._generate_search_queries(query)\n",
        "\n",
        "            # 3. Perform web search and add results to vector store\n",
        "            for i, search_query in enumerate(search_queries):\n",
        "                print(f\"  Search query {i+1}/{len(search_queries)}: {search_query}\")\n",
        "                documents = self.web_retriever.search(search_query)\n",
        "                print(f\"  Found {len(documents)} documents\")\n",
        "                for doc in documents:\n",
        "                    self.vector_store.add_document(doc)\n",
        "\n",
        "            # 4. Search again with new information\n",
        "            search_results = self.vector_store.similarity_search(query)\n",
        "\n",
        "        # 5. Format context from search results\n",
        "        if search_results:\n",
        "            context = self._format_context(search_results)\n",
        "            print(f\"Using {len(search_results)} relevant chunks of information\")\n",
        "        else:\n",
        "            context = \"No specific information found. Generating response based on general knowledge.\"\n",
        "            print(\"No specific information found in vector store\")\n",
        "\n",
        "        # 6. Generate answer using LLM\n",
        "        print(\"Generating answer using LLM...\")\n",
        "        prompt = f\"\"\"\n",
        "        Answer the following question based on the provided context. If the information is not in the context, say so.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "\n",
        "        answer = self.llm.generate(prompt)\n",
        "\n",
        "        # 7. Store interaction in memory\n",
        "        interaction = {\n",
        "            \"query\": query,\n",
        "            \"context\": context,\n",
        "            \"answer\": answer,\n",
        "            \"timestamp\": time.time()\n",
        "        }\n",
        "        self.add_to_memory(interaction)\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": answer,\n",
        "            \"sources\": [r[\"chunk\"][\"metadata\"].get(\"source\") for r in search_results] if search_results else []\n",
        "        }\n",
        "\n",
        "    def recursive_research(self, query: str, max_depth: int = 2) -> Dict[str, Any]:\n",
        "        \"\"\"Perform recursive research to answer complex queries\"\"\"\n",
        "        depth = 0\n",
        "        findings = []\n",
        "        current_query = query\n",
        "\n",
        "        print(f\"\\nüîç Starting recursive research on: {query}\")\n",
        "\n",
        "        while depth < max_depth:\n",
        "            print(f\"\\nüìö Research iteration {depth+1}/{max_depth} - Query: {current_query}\")\n",
        "\n",
        "            # Process the current query\n",
        "            result = self.process_query(current_query)\n",
        "            findings.append(result)\n",
        "\n",
        "            print(f\"\\n‚úì Found: {result['answer'][:100]}...\")\n",
        "\n",
        "            # Generate follow-up query based on findings\n",
        "            print(\"\\nü§î Generating follow-up question...\")\n",
        "\n",
        "            follow_up_prompt = f\"\"\"\n",
        "            Based on what we've learned so far about \"{query}\",\n",
        "            what important follow-up question should we research next?\n",
        "\n",
        "            Current findings: {result['answer']}\n",
        "\n",
        "            Return ONLY the follow-up question, nothing else.\n",
        "            \"\"\"\n",
        "\n",
        "            next_query = self.llm.generate(follow_up_prompt)\n",
        "\n",
        "            # Clean up the generated query\n",
        "            next_query = re.sub(r'^[^a-zA-Z0-9]+', '', next_query)  # Remove leading non-alphanumeric chars\n",
        "            next_query = next_query.split('\\n')[0].strip()  # Take first line only\n",
        "\n",
        "            print(f\"üìù Follow-up question: {next_query}\")\n",
        "\n",
        "            # Break if we're not getting meaningful follow-ups\n",
        "            if len(next_query) < 10 or next_query.lower() == current_query.lower():\n",
        "                print(\"No meaningful follow-up questions. Ending research cycle.\")\n",
        "                break\n",
        "\n",
        "            current_query = next_query\n",
        "            depth += 1\n",
        "\n",
        "        # Synthesize final answer from all findings\n",
        "        print(\"\\nüß† Synthesizing comprehensive answer...\")\n",
        "\n",
        "        synthesis_prompt = f\"\"\"\n",
        "        Synthesize a comprehensive answer to the original question based on all research findings.\n",
        "\n",
        "        Original question: {query}\n",
        "\n",
        "        Research findings:\n",
        "        {json.dumps([f['answer'] for f in findings], indent=2)}\n",
        "\n",
        "        Comprehensive answer:\n",
        "        \"\"\"\n",
        "\n",
        "        final_answer = self.llm.generate(synthesis_prompt)\n",
        "\n",
        "        return {\n",
        "            \"original_query\": query,\n",
        "            \"final_answer\": final_answer,\n",
        "            \"research_path\": [{\"query\": f[\"query\"], \"answer\": f[\"answer\"]} for f in findings]\n",
        "        }"
      ],
      "metadata": {
        "id": "OlAte4WEjmi5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Agent Implementation**\n",
        "\n",
        "Now we implement the main Agent class that orchestrates the entire process:"
      ],
      "metadata": {
        "id": "4w2wCY4RiIP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AgentRAG:\n",
        "    \"\"\"Main agent class that orchestrates the RAG process\"\"\"\n",
        "\n",
        "    def __init__(self, config: AgentConfig = None):\n",
        "        print(\"Initializing AgentRAG...\")\n",
        "        self.config = config or AgentConfig()\n",
        "        self.vector_store = VectorStore(self.config)\n",
        "        self.web_retriever = WebRetriever(self.config)\n",
        "        self.llm = LLMInterface(self.config)\n",
        "        self.memory = deque(maxlen=10)  # Short-term memory for conversation\n",
        "        print(\"AgentRAG initialized!\")\n",
        "\n",
        "    def add_to_memory(self, item: Dict[str, Any]) -> None:\n",
        "        \"\"\"Add an interaction to the agent's memory\"\"\"\n",
        "        self.memory.append(item)\n",
        "\n",
        "    def _format_context(self, chunks: List[Dict[str, Any]]) -> str:\n",
        "        \"\"\"Format retrieved chunks into context for the LLM\"\"\"\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"Source: {chunk['chunk']['metadata'].get('source', 'Unknown')}\\n\"\n",
        "            f\"Content: {chunk['chunk']['text']}\"\n",
        "            for chunk in chunks\n",
        "        ])\n",
        "        return context\n",
        "\n",
        "    def _generate_search_queries(self, user_query: str) -> List[str]:\n",
        "        \"\"\"Generate search queries based on the user query\"\"\"\n",
        "        # In production, use LLM to generate these\n",
        "        base_query = user_query.strip(\"?\").lower()\n",
        "        return [\n",
        "            base_query,\n",
        "            f\"latest information about {base_query}\",\n",
        "            f\"{base_query} explanation\"\n",
        "        ]\n",
        "\n",
        "    def _needs_web_search(self, query: str, results: List[Dict[str, Any]]) -> bool:\n",
        "        \"\"\"Determine if web search is needed based on query and existing results\"\"\"\n",
        "        if not results:\n",
        "            return True\n",
        "\n",
        "        # Check if results are relevant enough\n",
        "        avg_similarity = sum(r[\"similarity\"] for r in results) / len(results) if results else 0\n",
        "        return avg_similarity < 0.75"
      ],
      "metadata": {
        "id": "LN40MnYjiFlA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Query Processing Method**\n",
        "\n",
        "This is the main method that processes user queries:"
      ],
      "metadata": {
        "id": "scQd8_l4iPJu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def process_query(self, query: str) -> Dict[str, Any]:\n",
        "        \"\"\"Process a user query through the full agentic RAG pipeline\"\"\"\n",
        "        # 1. Check existing knowledge\n",
        "        search_results = self.vector_store.similarity_search(query)\n",
        "\n",
        "        # 2. Always perform web search for the first query or if needed\n",
        "        if not search_results or self._needs_web_search(query, search_results):\n",
        "            print(\"Retrieving information from the web...\")\n",
        "            search_queries = self._generate_search_queries(query)\n",
        "\n",
        "            # 3. Perform web search and add results to vector store\n",
        "            for i, search_query in enumerate(search_queries):\n",
        "                print(f\"  Search query {i+1}/{len(search_queries)}: {search_query}\")\n",
        "                documents = self.web_retriever.search(search_query)\n",
        "                print(f\"  Found {len(documents)} documents\")\n",
        "                for doc in documents:\n",
        "                    self.vector_store.add_document(doc)\n",
        "\n",
        "            # 4. Search again with new information\n",
        "            search_results = self.vector_store.similarity_search(query)\n",
        "\n",
        "        # 5. Format context from search results\n",
        "        if search_results:\n",
        "            context = self._format_context(search_results)\n",
        "            print(f\"Using {len(search_results)} relevant chunks of information\")\n",
        "        else:\n",
        "            context = \"No specific information found. Generating response based on general knowledge.\"\n",
        "            print(\"No specific information found in vector store\")\n",
        "\n",
        "        # 6. Generate answer using LLM\n",
        "        print(\"Generating answer using LLM...\")\n",
        "        prompt = f\"\"\"\n",
        "        Answer the following question based on the provided context. If the information is not in the context, say so.\n",
        "\n",
        "        Context:\n",
        "        {context}\n",
        "\n",
        "        Question: {query}\n",
        "\n",
        "        Answer:\n",
        "        \"\"\"\n",
        "\n",
        "        answer = self.llm.generate(prompt)\n",
        "\n",
        "        # 7. Store interaction in memory\n",
        "        interaction = {\n",
        "            \"query\": query,\n",
        "            \"context\": context,\n",
        "            \"answer\": answer,\n",
        "            \"timestamp\": time.time()\n",
        "        }\n",
        "        self.add_to_memory(interaction)\n",
        "\n",
        "        return {\n",
        "            \"query\": query,\n",
        "            \"answer\": answer,\n",
        "            \"sources\": [r[\"chunk\"][\"metadata\"].get(\"source\") for r in search_results] if search_results else []\n",
        "        }"
      ],
      "metadata": {
        "id": "Q_aAIHULiMnW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recursive Research Method**\n",
        "\n",
        "This method enables deeper research through follow-up questions:"
      ],
      "metadata": {
        "id": "b6fLl6WLiVxP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def recursive_research(self, query: str, max_depth: int = 2) -> Dict[str, Any]:\n",
        "        \"\"\"Perform recursive research to answer complex queries\"\"\"\n",
        "        depth = 0\n",
        "        findings = []\n",
        "        current_query = query\n",
        "\n",
        "        print(f\"\\nüîç Starting recursive research on: {query}\")\n",
        "\n",
        "        while depth < max_depth:\n",
        "            print(f\"\\nüìö Research iteration {depth+1}/{max_depth} - Query: {current_query}\")\n",
        "\n",
        "            # Process the current query\n",
        "            result = self.process_query(current_query)\n",
        "            findings.append(result)\n",
        "\n",
        "            print(f\"\\n‚úì Found: {result['answer'][:100]}...\")\n",
        "\n",
        "            # Generate follow-up query based on findings\n",
        "            print(\"\\nü§î Generating follow-up question...\")\n",
        "\n",
        "            follow_up_prompt = f\"\"\"\n",
        "            Based on what we've learned so far about \"{query}\",\n",
        "            what important follow-up question should we research next?\n",
        "\n",
        "            Current findings: {result['answer']}\n",
        "\n",
        "            Return ONLY the follow-up question, nothing else.\n",
        "            \"\"\"\n",
        "\n",
        "            next_query = self.llm.generate(follow_up_prompt)\n",
        "\n",
        "            # Clean up the generated query\n",
        "            next_query = re.sub(r'^[^a-zA-Z0-9]+', '', next_query)  # Remove leading non-alphanumeric chars\n",
        "            next_query = next_query.split('\\n')[0].strip()  # Take first line only\n",
        "\n",
        "            print(f\"üìù Follow-up question: {next_query}\")\n",
        "\n",
        "            # Break if we're not getting meaningful follow-ups\n",
        "            if len(next_query) < 10 or next_query.lower() == current_query.lower():\n",
        "                print(\"No meaningful follow-up questions. Ending research cycle.\")\n",
        "                break\n",
        "\n",
        "            current_query = next_query\n",
        "            depth += 1\n",
        "\n",
        "        # Synthesize final answer from all findings\n",
        "        print(\"\\nüß† Synthesizing comprehensive answer...\")\n",
        "\n",
        "        synthesis_prompt = f\"\"\"\n",
        "        Synthesize a comprehensive answer to the original question based on all research findings.\n",
        "\n",
        "        Original question: {query}\n",
        "\n",
        "        Research findings:\n",
        "        {json.dumps([f['answer'] for f in findings], indent=2)}\n",
        "\n",
        "        Comprehensive answer:\n",
        "        \"\"\"\n",
        "\n",
        "        final_answer = self.llm.generate(synthesis_prompt)\n",
        "\n",
        "        return {\n",
        "            \"original_query\": query,\n",
        "            \"final_answer\": final_answer,\n",
        "            \"research_path\": [{\"query\": f[\"query\"], \"answer\": f[\"answer\"]} for f in findings]\n",
        "        }"
      ],
      "metadata": {
        "id": "2lNAtfGRiYmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Main Execution Function**"
      ],
      "metadata": {
        "id": "qnOPrxlRieJU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_agent():\n",
        "    # Check for API keys\n",
        "    openai_key = os.environ.get(\"OPENAI_API_KEY\")\n",
        "    serper_key = os.environ.get(\"SERPER_API_KEY\")\n",
        "\n",
        "    if not openai_key or openai_key == \"your-openai-api-key-here\":\n",
        "        print(\"‚ö†Ô∏è Warning: OpenAI API key not found or not set.\")\n",
        "        print(\"Please set your OpenAI API key in the environment variables.\")\n",
        "        return\n",
        "\n",
        "    if not serper_key or serper_key == \"your-serper-api-key-here\":\n",
        "        print(\"‚ö†Ô∏è Warning: Serper API key not found or not set.\")\n",
        "        print(\"Please set your Serper API key in the environment variables.\")\n",
        "        return\n",
        "\n",
        "    # Initialize the agent\n",
        "    config = AgentConfig(\n",
        "        api_key=openai_key,\n",
        "        serper_api_key=serper_key\n",
        "    )\n",
        "\n",
        "    agent = AgentRAG(config)\n",
        "\n",
        "    # Get query from user\n",
        "    query = input(\"Enter your research question: \")\n",
        "\n",
        "    if not query:\n",
        "        query = \"What are the environmental impacts of electric vehicles compared to gas vehicles?\"\n",
        "        print(f\"Using default question: {query}\")\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"üîç PROCESSING QUERY: {query}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "\n",
        "    try:\n",
        "        # Process with regular search first for a quick answer\n",
        "        print(\"Performing initial research...\")\n",
        "        basic_result = agent.process_query(query)\n",
        "\n",
        "        print(\"\\nüìù INITIAL ANSWER:\")\n",
        "        print(\"-\" * 80)\n",
        "        print(basic_result[\"answer\"])\n",
        "        print(\"-\" * 80)\n",
        "\n",
        "        if basic_result[\"sources\"]:\n",
        "            print(\"\\nüìö SOURCES:\")\n",
        "            for i, source in enumerate(basic_result[\"sources\"], 1):\n",
        "                print(f\"{i}. {source}\")\n",
        "\n",
        "        # Ask user if they want more in-depth research\n",
        "        user_input = input(\"\\nü§î Would you like deeper research on this topic? (y/n): \")\n",
        "\n",
        "        if user_input.lower() in ['y', 'yes']:\n",
        "            print(\"\\nPerforming deeper recursive research. This may take a few minutes...\\n\")\n",
        "            result = agent.recursive_research(query, max_depth=2)\n",
        "\n",
        "            print(\"\\nüéØ COMPREHENSIVE ANSWER:\")\n",
        "            print(\"=\" * 80)\n",
        "            print(result[\"final_answer\"])\n",
        "            print(\"=\" * 80)\n",
        "\n",
        "            print(\"\\nüîç RESEARCH PATH:\")\n",
        "            for i, step in enumerate(result[\"research_path\"]):\n",
        "                print(f\"\\nStep {i+1}:\")\n",
        "                print(f\"Query: {step['query']}\")\n",
        "                print(f\"Finding: {step['answer'][:300]}...\")\n",
        "                if len(step['answer']) > 300:\n",
        "                    print(\"...\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n‚ùå Error during research: {str(e)}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "    print(\"\\n‚ú® Done!\")"
      ],
      "metadata": {
        "id": "86RYJoiKiaUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run this to start the agent\n",
        "run_agent()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv2tGe6HipDu",
        "outputId": "ced70e1c-aa52-4cc2-90c8-52196d5c8858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing AgentRAG...\n",
            "Loading embedding model on cpu...\n",
            "Embedding model loaded successfully!\n",
            "AgentRAG initialized!\n",
            "Enter your research question: what is the purpose of agents\n",
            "\n",
            "================================================================================\n",
            "üîç PROCESSING QUERY: what is the purpose of agents\n",
            "================================================================================\n",
            "\n",
            "Performing initial research...\n",
            "Retrieving information from the web...\n",
            "  Search query 1/3: what is the purpose of agents\n",
            "  Found 3 documents\n",
            "  Search query 2/3: latest information about what is the purpose of agents\n",
            "  Found 3 documents\n",
            "  Search query 3/3: what is the purpose of agents explanation\n",
            "  Found 3 documents\n",
            "No specific information found in vector store\n",
            "Generating answer using LLM...\n",
            "\n",
            "üìù INITIAL ANSWER:\n",
            "--------------------------------------------------------------------------------\n",
            "The purpose of agents can vary depending on the context in which they are used. In general, agents are individuals or entities that act on behalf of others to represent their interests, negotiate deals, provide services, or perform specific tasks. In various fields such as real estate, entertainment, and business, agents play a crucial role in facilitating transactions and relationships between parties.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "ü§î Would you like deeper research on this topic? (y/n): n\n",
            "\n",
            "‚ú® Done!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H5DdqQvRis_g"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}